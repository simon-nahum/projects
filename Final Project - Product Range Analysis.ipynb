{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Commerce Product Range Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to analyze the product range of an online store. \n",
    "\n",
    "To do so we shall analyze the following: \n",
    "\n",
    " - **Basket Analysis**: We will find items that are typically bought together in the same invoice. \n",
    " \n",
    " - **Assortment**: Using keywords in the product description we will try to identify the main categories of products. \n",
    " \n",
    " - **Returned Items**: We will analyze the distribution of the ratio of returns of items and identify the products with highest ratio. \n",
    " \n",
    " - **Popularity**: We will identify the most popular items, and analyze distribution of popularity. \n",
    " \n",
    " - **Seasonality**: We will plot revenues and average invoice sizes by date to see if there is any seasonal pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link for PDF Presentation\n",
    "\n",
    "https://drive.google.com/file/d/1MuLi6QFhrnAz6mI1vVJV3-kY0eUbZFk0/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link for Tableau Dashboard\n",
    "\n",
    "https://public.tableau.com/views/FinalProjectDashboard_16437228832330/DailyPurchases?:language=en-GB&publish=yes&:display_count=n&:origin=viz_share_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of the data:**\n",
    "\n",
    "The dataset contains the transaction history of an online store that sells household goods.\n",
    "\n",
    "The file `ecommerce_dataset_us.csv` contains the following columns:\n",
    "\n",
    "`InvoiceNo` — order identifier\n",
    "\n",
    "`StockCode` — item identifier\n",
    "\n",
    "`Description` — item name\n",
    "\n",
    "`Quantity`\n",
    "\n",
    "`InvoiceDate` — order date\n",
    "\n",
    "`UnitPrice` — price per item\n",
    "\n",
    "`CustomerID`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import datetime as dt \n",
    "\n",
    "# !pip install mlxtend\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# !pip install pymystem3\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('C:/Users/tucan/OneDrive/Desktop/DATA SCIENCE/Final Project/data.csv', sep = '\\t', engine = 'python')\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/ecommerce_dataset_us.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage = 'deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we notice that Quantity and Unit Price data have abnormal data: both negative and extremely high values. This issue will be addressed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that CustomerID and Description columns have missing values. Let's see what the share of missing values is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of Missing Values:')\n",
    "(df.isna().sum() / len(df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quarter of all data is lacking a value in the Customer ID column. Clearly we cannot drop such an amount of data. \n",
    "\n",
    "We saw earlier that Customer IDs range from 12346 to 18287. If we assign a \"0\" to all missing customer IDs, we can identify them as \"unknown customer\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing Customer IDs with a 0\n",
    "\n",
    "df.CustomerID.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Description column has a 0.27% of missing values. This is a relatively small amount of data, so we can simply delete these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing description\n",
    "\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of Missing Values:')\n",
    "(df.isna().sum() / len(df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All missing values have been removed or replaced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that some data types need to be converted. \n",
    "\n",
    "Customer ID column is a float, although it should not have any decimals. Let's convert it to integer type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CustomerID'] = df.CustomerID.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoice date column has to be converted from object to datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By converting data types, we reduced memory usage to a fraction of the original size: from 158 MB down to 31 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating columns for Date, Month and Revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create columns for the date and month of the invoice using data from Invoice Date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Date and Month Columns\n",
    "\n",
    "df['date'] = df['InvoiceDate'].dt.date\n",
    "df['month'] = df['InvoiceDate'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the total revenue by multiplying Unit Price and Quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['revenue'] = df['UnitPrice'] * df['Quantity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that the Quantity Column has some abnormal data. Let's take a closer look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Quantity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum and minimum values have the same absolute value but opposite signs, and have an unreasonably high value. Let's view these rows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Quantity == 80995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Quantity == -80995]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows are almost identical, except the Invoice number for the row with the negative value has a \"C\" before the number, probably standing for \"cancelled\". The positive values could be a human errors. If that is the case, the negative value is used to cancel out the error. Negative values could also indicate returned items or discounts. Let's store all rows with negative quantities in a separate dataframe, called returns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame for Returned Items and Cancelled Invoices\n",
    "\n",
    "returns = df[df.Quantity < 0]\n",
    "returns.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of rows that have negative quantity values:')\n",
    "((len(returns) / len(df))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove these negative values from our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Quantity > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our data looks like now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Quantity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed all negative values, but there still are many abnormal positive values. For our analysis we should filter out these errors. To decide what data is to be considered abnormal, let's look at the 99.5% quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Quantity.quantile(q=0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99.5% of our rows have less than 160 units of a specific item on a single invoice. Let's consider any quantity beyond 160 units as an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Quantity'] <= 160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing rows with more than 160 units we removed 0.5% of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Quantity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that Unit Price also have some abnormal data. Let's have a closer look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.UnitPrice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we removed all negative quantity values and unusually high quantities, our unit price column still has abnormal values, including both negative and unusually high prices. Let's have a look at these values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.UnitPrice < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 2 rows with negative price, almost identical, that are an adjustment to a bad debt. Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.UnitPrice >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many rows have a unit price equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.UnitPrice == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.UnitPrice == 0].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 555 rows with a Unit Price of 0. These items might have been given as a gift or a special promotion, or could be simply a human error. Anyhow, they represent only 0.001% of our data, so it is reasonable to remove them as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.UnitPrice > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at abnormally high values for quantity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['UnitPrice'] > 3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that abnormally high unit prices are either Amazon or Postage Fees, or Invoices that were made manually. Since the latter lack details about the items sold, we won't use them for our analysis. Let's see how many invoices were made manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['StockCode'] == 'M'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 314 such invoices, let's remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['StockCode'] != 'M']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for Amazon, Postage, Dotcom and bad debt adjustments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of rows for Amazon Fee:')\n",
    "len(df[df['StockCode'] == 'AMAZONFEE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['StockCode'] != 'AMAZONFEE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of rows for Postage:')\n",
    "len(df[df['StockCode'] == 'POST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['StockCode'] != 'POST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of rows for Dotcom Postage:')\n",
    "len(df[df['StockCode'] == 'DOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['StockCode'] != 'DOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of Bad Debt Adjustments:')\n",
    "len(df[df['StockCode'] == 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['StockCode'] != 'B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have removed all these rows that are not relevant for our analysis, let's look again at our data, sorting by unit price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='UnitPrice', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the most expensive item is the Picnic Basket, costing 649.5, followed by the kitchen cabinet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is ready to be analyzed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by finding items that are typically bought together in the same invoice.\n",
    "\n",
    "To do so we will first group our data by Invoice and item description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = (df.groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our basket analysis we do not need to know the exact number of units of a specific item in a basket. Rather, we only need to know if the item is present or not in the invoice. Let's turn the quantity values into boolean: 0 if the item is not in the invoice and 1 if it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "basket_sets = basket.applymap(encode_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the apriori algorithm to create a list of items and their support factor. We shall use a minimum support value of 0.03, meaning that we consider only items that appear in at least 3% of invoices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(basket_sets, min_support=0.03, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the association rules algorithm to find the support, confidence and lift factors for all the item combinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "rules.sort_values('confidence', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the combinations of items typically bought together we notice that there are mainly three types of items combinations: Bags, Teacups, and Alarm Clocks. \n",
    "When a customer buys teacups and saucers, he/she usually buys two (or more) different colors of this item. Same for bags and alarm clocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next step is to try and categorize the items based on their description. \n",
    "\n",
    "To do so we will first see what are ten items that appear more often in our data. \n",
    "\n",
    "We will also use lemmatization to count the number of times each word appears in the description column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Ten Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Description.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list_full = df['Description'].unique()\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "string = '; '.join(product_list_full)\n",
    "\n",
    "lemmas = m.lemmatize(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of times each word appears\n",
    "\n",
    "counter = Counter(lemmas)\n",
    "# print(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data from the top ten items and the most common words that appear in descriptions we will create a function that assigns a category to an item if it contains a specific keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category(description):   \n",
    "    if 'BOX' in description:\n",
    "        return 'BOXES'\n",
    "    if 'DECORATION' in description or 'ORNAMENT' in description:\n",
    "        return 'DECORATIONS AND ORNAMENTS'\n",
    "    if 'BRACELET' in description or 'NECKLACE' in description or 'EARRINGS' in description:\n",
    "        return 'JEWELRY'  \n",
    "    if 'CARD' in description:\n",
    "        return 'CARDS'\n",
    "    if 'GAME' in description:\n",
    "        return 'GAMES'\n",
    "    if 'CLOCK' in description:\n",
    "        return 'CLOCKS'\n",
    "    if 'LIGHT' in description:\n",
    "        return 'LIGHTS AND LIGHT HOLDERS'\n",
    "    if 'BAG' in description:\n",
    "        return 'BAGS'\n",
    "    if 'CUP' in description:\n",
    "        return 'CUPS'\n",
    "    if 'MUG' in description:\n",
    "        return 'CUPS'\n",
    "    if 'TEA' in description:\n",
    "        return 'CUPS'\n",
    "    if 'COFFEE' in description:\n",
    "        return 'CUPS'\n",
    "    if  'CABINET' in description:\n",
    "        return 'CABINET'\n",
    "    if 'CHALK' in description:\n",
    "        return 'CHALKBOARDS'\n",
    "    if 'CUTLERY' in description:\n",
    "        return 'CUTLERY'\n",
    "    if 'CAKE' in description:\n",
    "        return 'CAKE STANDS, CASES AND TINS'\n",
    "    if 'JAM' in description:\n",
    "        return 'JAM MAKING SETS'\n",
    "    if 'BUNTING' in description:\n",
    "        return 'BUNTING'\n",
    "    if 'FRAME' in description:\n",
    "        return 'FRAMES'\n",
    "    if 'BOWL' in description:\n",
    "        return 'BOWLS'\n",
    "    if 'SHOPPER' in description:\n",
    "        return 'SHOPPERS'  \n",
    "    if 'HANGER' in description:\n",
    "        return 'HANGERS'  \n",
    "    if 'LANTERN' in description:\n",
    "        return 'LIGHTS AND LIGHT HOLDERS'\n",
    "    if 'BOTTLE' in description:\n",
    "        return 'BOTTLES'\n",
    "    if 'DOLL' in description:\n",
    "        return 'TOYS AND DOLLS' \n",
    "    if 'TOY' in description:\n",
    "        return 'TOYS AND DOLLS'\n",
    "    if 'DOORMAT' in description:\n",
    "        return 'DOORMATS'\n",
    "    if 'CANDLE' in description:\n",
    "        return 'CANDLES AND CANDLE HOLDERS'\n",
    "    if 'PEN' in description or 'PENCIL' in description or 'STATIONERY' in description:\n",
    "        return 'STATIONERY'\n",
    "    else:\n",
    "        return 'OTHER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the function to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'] = df['Description'].apply(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the percentage of items belonging to each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((df['Category'].value_counts()/len(df))*100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have identified the 20 most common categories of items, only 54% of items have been categorized. \n",
    "The most common category is bags, which accounts for almost 10% of all items, followed by cups, boxes and lights, which are approximately 5% each. \n",
    "\n",
    "Let's apply the same function to the returns dataframe. First, though, we need to filter the returns dataframe like we did for the main dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.UnitPrice.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[returns['UnitPrice'] > 649.5].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(returns[returns['UnitPrice'] > 649.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that the most expensive item costs 649.5. Anything beyond that value is either a postage fee or a manual invoice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rows correspond to about 0.1 % of our data. Let's filter them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns[returns['UnitPrice'] <= 649.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data for Quantity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.Quantity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(returns[returns['Quantity'] < -160])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our main dataframe we removed all invoices for more than 160 units of an item. In returned items data about 2% of our rows have quantities lower than -160. Let's remove these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns[returns['Quantity'] > -160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the categorizing function to returned items data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns['Category'] = returns['Description'].apply(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((returns['Category'].value_counts()/len(returns))*100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bags are almost 10% of our main data but only 7.39% of returned items. Cups, on the contrary, were only 5% of items in our main data but 7.65% for returned items. Also cake-related items, boxes and lights had a higher ratio of returns compared to their share in the main data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to analyze the seasonality of sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at daily revenues and monthly revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x = 'date', y ='revenue', title = 'Daily Revenues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the daily revenues we notice the following: \n",
    "\n",
    "- There are two gaps with zero sales durning Christmas and Easter holidays, the former starting from the 23rd of December to the 2nd of January, the latter from the 20th to the 24th of April. \n",
    "\n",
    "- From a weekly point of view, there are no sales on Sundays. \n",
    "\n",
    "Our data covers a period of slightly more than one year (from the 29th of November 2018 until December 7th 2019). \n",
    "\n",
    "Let's view our revenue histogram with 13 bins to see if there is any seasonal pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x = 'date', y ='revenue', title = 'Monthly Revenue', nbins = 13)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring November 2018 and December 2019 (since data for these months is incomplete), we can clearly see that November is the month with the highest revenue (84.000), followed by October with 61.000. All other months have a total revenue ranging from 25.000 to 46.000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Popularity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the general distribution of popularity of each item: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_item = df.pivot_table(index = 'Description', \n",
    "                                 values = ['InvoiceNo', 'UnitPrice'], \n",
    "                                 aggfunc = {'InvoiceNo':'count',\n",
    "                                            'UnitPrice' : 'mean'}).round(2)\n",
    "\n",
    "grouped_by_item.rename(columns = {'InvoiceNo' : 'Purchases'}, \n",
    "                       inplace = True)\n",
    "\n",
    "grouped_by_item = grouped_by_item.sort_values('Purchases', ascending = False).reset_index()\n",
    "\n",
    "grouped_by_item.Purchases.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An item is purchased on average 130 times a year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the ten items that appear most often in invoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Description.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the items that sold the most units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity = df.pivot_table(index = ['StockCode','Description'],\n",
    "                       values = 'Quantity',\n",
    "                       aggfunc = 'sum')\n",
    "\n",
    "quantity.sort_values(by = 'Quantity', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing these two lists we can see some items that do not appear in the top ten in the list of items per invoice, but appear in the top ten when considering number of units sold, like the WORLD WAR 2 GLIDERS and POPCORN HOLDERS. Let's have a look at the average number of units of each item per invoice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_quantity = df.pivot_table(index = ['StockCode','Description'],\n",
    "                       values = 'Quantity',\n",
    "                       aggfunc = 'mean').astype('int')\n",
    "avg_quantity.rename(columns={'Quantity' : 'Average Number of Units'}, inplace=True)\n",
    "avg_quantity.sort_values(by = 'Average Number of Units', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_quantity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The item than on average sells the highest number of units is Mini Highlighter, with an average of 120 units per invoice and we see several other stationary items in the top ten of the list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Range Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze the distribution of price per unit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.UnitPrice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the most expensive item costs 649.5, which is about 200 times the average cost per unit. \n",
    "\n",
    "Let's calculate the 99% quantile: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.UnitPrice.quantile(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 1% of items are more expensive than 16.63. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which are the top ten most expensive items: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df.pivot_table(index = ['StockCode','Description'],\n",
    "                       values = 'UnitPrice').round(2)\n",
    "# avg_quantity.rename(columns={'UnitPrice' : 'Price per Unit'}, inplace=True)\n",
    "price.sort_values(by = 'UnitPrice', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second most expensive item, the Sideboard, is only a quarter of the price of the most expensive item, the Picnic basket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a histogram to analyze the distribution of prices of items cheaper than 17, which as we saw are 99% of our products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(price[price['UnitPrice'] <= 17], x = 'UnitPrice',  title = 'Distribution of Item Prices', nbins = 16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately half of all items are within the price range of 1 and 3. A minority of items cost beyond 5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoice Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the variety of invoices by total revenue and number of items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoices = df.pivot_table(index = ['InvoiceNo', 'date'],\n",
    "                                 values = ['revenue', 'StockCode', 'Quantity', 'UnitPrice'],\n",
    "                                 aggfunc = {'revenue': 'sum',\n",
    "                                          'StockCode': 'count',\n",
    "                                          'Quantity' : 'sum',\n",
    "                                           'UnitPrice' : 'mean'})\n",
    "\n",
    "invoices.rename(columns= {'StockCode': 'Items',\n",
    "                        'Quantity' : 'Units',\n",
    "                         'UnitPrice' : 'Average Product Price'}, \n",
    "                        inplace = True)\n",
    "\n",
    "invoices.sort_values('revenue', ascending = False, inplace = True)\n",
    "\n",
    "invoices.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoices.revenue.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median invoice size is 300 and average size is 450. A few very large outliers are pulling the average higher than the median. As we can see the biggest invoice is almost 40.000. Let's compute the 95% quantile and plot a histogram for invoice sizes of 95% of invoices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoices.revenue.quantile(.95).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(invoices[invoices['revenue'] < 1321], \n",
    "                   x = 'revenue',  \n",
    "                   title = 'Distribution of Invoice Revenue', \n",
    "                   nbins = 20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The revenue range that has the highest number of invoices is between 250 and 350. \n",
    "\n",
    "Now let's analyze the seasonality of invoices by revenue and number of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_by_date = invoices.pivot_table(index = 'date',\n",
    "                                                values = ['revenue', 'Items', 'Units'],\n",
    "                                                aggfunc = 'mean').round(2).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(invoice_by_date, \n",
    "                   x = 'date',\n",
    "                   y = 'revenue',\n",
    "                   title = 'Seasonality of Invoice Size by Revenue', \n",
    "                   nbins = 50)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier there are short winter and Easter breaks were there are 0 sales, so that explains the two gaps with lower revenue. Also we know that October and November are the months with highest overall revenue, and we can see that the average revenue per invoice in that period is only slightly higher than the rest of the year, suggesting that October and November mainly have a larger number of customer, rather than the same number of customers spending more money compared to other times of the year. \n",
    "\n",
    "Although January is not one of the most profitable months from an overall revenue point of view, we notice the average revenue per invoice seems to be higher than average specifically in the period within the 7th and 19th of January. \n",
    "\n",
    "Now let's analyze the seasonality of invoice size by number of items bought. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoices.Items.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(invoice_by_date, \n",
    "                   x = 'date',\n",
    "                   y = 'Items',\n",
    "                   title = 'Seasonality of Invoice Size by Number of Items', \n",
    "                   nbins = 40)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the sum of average number of items per day we notice the peak in the period from the 6th to the 19th of January is even more obvious. We shall test later if this difference is statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returned Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a table for the number of returns per item: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_grouped_by_item = returns.pivot_table(index = 'Description', \n",
    "                                              values = ['InvoiceNo'], \n",
    "                                              aggfunc = 'count')\n",
    "\n",
    "returns_grouped_by_item.rename(columns = {'InvoiceNo' : 'Returns'}, inplace = True)\n",
    "\n",
    "returns_grouped_by_item = returns_grouped_by_item.sort_values('Returns', ascending = False).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the return ratio of each product, by merging the table of returned items with the list of all products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio = grouped_by_item.merge(returns_grouped_by_item, on = 'Description', how = 'left')\n",
    "\n",
    "return_ratio['return_ratio'] = ((return_ratio[\"Returns\"] / return_ratio[\"Purchases\"])*100).round(1)\n",
    "\n",
    "return_ratio.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by studying the general return ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio.return_ratio.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are some abnormally high ratios. Logically an item cannot be returned more than 100% of the times it was purchased. Let's view return ratios that are beyond 100%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio.sort_values(by=['return_ratio'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nine rows in returned items have a rate higher than 100%, but only six of these are actually products (the others are Manual, Samples and Bank Charges). These products all sold a very small number of units, and the returned units have probably been purchased before the analyzed period. So let's filter these rows and view the statistics after filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio = return_ratio[return_ratio['return_ratio'] <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio['return_ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the majority of items were not returned even once, and the average return ratio is 1.8%. \n",
    "\n",
    "Let's view the items that were returned the most: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio.sort_values(by=['return_ratio'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that products with a return ratio of 100% or 50% typically were purchased only once or twice. \n",
    "Let's find the 5% quantile for number of purchases, and view items that were bought more than that value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio['Purchases'].quantile(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% of items were purchased more than twice. Let's view the items with highest return ratio that sold more than twice: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio[return_ratio['Purchases'] > 2].sort_values(by=['return_ratio', 'Purchases'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the return ratio for the ten most popular items: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio.sort_values(by=['Purchases', 'return_ratio'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the ten most popular items, most have a very low return ratio, except two: REGENCY CAKESTAND 3 TIER and SET OF 3 CAKE TINS PANTRY DESIGN, both of them belonging to the \"cake\" category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the return ratio for the ten most expensive items: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio.sort_values(by=['UnitPrice'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the most expensive items, we notice two vintage kitchen cabinets, red and blue, have a very high return ratio. The vintage post office cabinet, however, has 0 returns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that there was a peak in the sum of the average number of items during the two week period from the 6th of January to the 19th. Now we will test if the average number of items per invoice in this period is significantly bigger than the average for the rest of the year.\n",
    "\n",
    "First let's create a dataframe for the period we want to test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = df[df['InvoiceDate'] > '2019-01-06']\n",
    "test_period = test_period[test_period['InvoiceDate'] <= '2019-01-20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the average number of items per invoice and their variance, for both test and control period.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period_items = test_period.groupby('InvoiceNo')['StockCode'].count()\n",
    "\n",
    "print('Average Number of Items per invoice in test period: {0:.1f}'.format(test_period_items.mean())) \n",
    "\n",
    "print('Variance of number of items in test period: {0:.1f}'.format(np.var(test_period_items)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Number of Items per invoice in whole data: {0:.1f}'.format(invoices.Items.mean()))\n",
    "\n",
    "print('Variance of number of items in whole data: {0:.1f}'.format(np.var(invoices.Items)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our hypothesis:\n",
    "\n",
    " - HO: There is no difference in number of items per invoice in the period from the 6th and the 19th of January compared to the average of the whole year. \n",
    "\n",
    " - H1: In the period between the 6th and 19th of January, invoices included a higher than average number of items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "results = st.ttest_ind(test_period_items, invoices.Items, equal_var = False) \n",
    "\n",
    "print('p-value:', results.pvalue) \n",
    "\n",
    "if (results.pvalue < alpha): \n",
    "    print(\"We reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"We can't reject the null hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the period we tested, invoices had on average 39 items. This value is significantly higher than the rest of the year, with an average of 27. \n",
    "\n",
    "However, we saw earlier that the sum of average revenues in January was actually one of the lowest of the year. This suggests that although the part of January we tested had more items in each invoice, the same period might not have a significantly higher average revenue per invoice. \n",
    "\n",
    "Let us compare the revenue of our test period with the average revenue per invoice for the whole year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period_revenue = test_period.groupby('InvoiceNo')['revenue'].sum()\n",
    "\n",
    "print('Average Revenue per invoice in test period: {0:.1f}'.format(test_period_revenue.mean())) \n",
    "print('Variance of revenue in test period: {0:.1f}'.format(np.var(test_period_revenue)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Revenue per invoice in whole data: {0:.1f}'.format(invoices.revenue.mean())) \n",
    "\n",
    "print('Variance of revenue in whole data: {0:.1f}'.format(np.var(invoices.revenue)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average invoice in the test period is larger than average, but is this difference statistically significant? Let's test the following hypothesis: \n",
    "\n",
    " - HO: The average revenue per invoice in the period between the 6th and the 19th of January is the same as the average invoice of the whole year\n",
    " - H1: In the period  between the 6th and the 19th of January the average revenue per invoice is higher than average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "results = st.ttest_ind(test_period_revenue, invoices.revenue, equal_var = False) \n",
    "\n",
    "print('p-value:', results.pvalue) \n",
    "\n",
    "if (results.pvalue < alpha): \n",
    "    print(\"We reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"We can't reject the null hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is not statistically significant, meaning that the period in January we are testing is not characterized buy larger invoices, but just invoices with a higher number of items. The P-value, however, is just slightly higher than our level of statistical significance, 0.05. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The two week period we tested is characterized by invoices with a larger variety, but not significantly higher revenues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusions\n",
    "\n",
    "\n",
    "#### Preparing the Data\n",
    "\n",
    "After studying and preprocessing our data, we removed from our data all rows that do not correspond to a specific products.  Some of these rows were charges for different kinds of fees like posting or Amazon fees. Some rows were manual invoices, that do not have a list of products, but just the total sum of the purchase. \n",
    "Then we found rows with negative prices and identified them as returned items, and stored them in a separate table. \n",
    "\n",
    "#### Categorization\n",
    "\n",
    "We then tried to identify the 20 most common categories of products using the most common keywords present in the item description. We managed to categorize slightly more than half of all items. The most common categories are Bags, Cups and Boxes.\n",
    "\n",
    "#### Price Range\n",
    "\n",
    "Most of the products are very cheap, within the range of 1 and 3. Only 1% of items cost more than 16.63. The most expensive product costs 649.50: the PICNIC BASKET WICKER 60 PIECES. \t\n",
    "\n",
    "#### Popularity\n",
    "\n",
    "We saw that the average product appears 130 times per year in invoices, but the median value is only 63, indicating that some very popular items are being sold much more than average. The most popular item, the WHITE HANGING HEART T-LIGHT HOLDER appears 2304 times in invoices. We then checked the average number of units bought for the same item in an invoice, and found that, on average, 6.7 units of an item are sold per invoice.    \n",
    "\n",
    "#### Returned Items \n",
    "\n",
    "We compared the ranking of the most common categories with the shares of categories amongst returned items and found that bags have a relatively lower chance of being returned, while cups, boxes and cake-related items tend to have a slightly higher chance of being returned.\n",
    "We analyzed the ratio of return of all items, and found that the average return ratio is 1.8 %, and the majority of products were not returned even once. We selected items that sold at least 10 times during the analyzed year and viewed which of these products had the highest return ratios. The item with the highest ratio was DANISH ROSE BEDSIDE CABINET, with a 50% return ratio. Of the top ten products, three of them belong to the cabinet category. We then checked the ratio amongst the most popular items, and found that most of these had very low ratios. Of these products, the one with the highest ratio was REGENCY CAKESTAND 3 TIER with 9% being returned. When looking at the ten most expensive items, we notice again two items belonging to the cabinet category with very high return ratios: the VINTAGE KITCHEN CABINETS in blue and red. \n",
    "\n",
    "#### Basket Analysis\n",
    "\n",
    "We performed a Basket Analysis and found the items that have are typically being bought together in the same purchase. We noticed that the items that are most typically being bought together are the same product in different colours. These products are: \n",
    "\n",
    " - REGENCY TEACUPS AND SAUCERS \n",
    " - ALARM CLOCK BAKELIKE\n",
    " - JUMBO BAG RETROSPOT\n",
    "\n",
    "It could be a good idea to offer customers a special deal on an assortment of different colors for these products.\n",
    "\n",
    "#### Seasonality\n",
    "\n",
    "We analysed the overall seasonality of revenues and found that the most profitable months are November and October. \n",
    "We also noticed that there are zero sales on Sundays. The same is true for winter and Easter holidays.\n",
    "We then anayzed the seasonality of the average size of invoices by average number of items and revenue, and noticed a peak on the two week period from the 6th to the 19th of January. We checked if this increase in average number of items and revenue is significantly higher than the yearly average, and we found that the number of items is significantly higher, while average revenues were not. There could be two possible explanations for this: \n",
    "\n",
    " - 1) During this period customers tend to buy cheaper items with a larger variety\n",
    " \n",
    " - 2) During this period customers tend to buy more items with the same average price but less units of each item\n",
    " \n",
    "It could be interesting to look deeper into this aspect. This could be a subject for deeper analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources: \n",
    "\n",
    "For the Basket Analysis I was helped by the following articles: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-market-basket-analysis/\n",
    "\n",
    "https://pbpython.com/market-basket-analysis.html\n",
    "\n",
    "https://analyticsindiamag.com/beginners-guide-to-understanding-apriori-algorithm-with-implementation-in-python/\n",
    "\n",
    "https://medium.com/@jihargifari/how-to-perform-market-basket-analysis-in-python-bd00b745b106"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
